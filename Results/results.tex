%##########################################################
\section{Results}
%##########################################################
\label{sec:experiments}

The experimental results are presented by following the next sequence:
\begin{itemize}
\item  We started by evaluating the classification performance in FHR records, wihtout any preprocessing, using only the NCD similarity criterion and a nearest neighbor classifier. This experiment evaluated the performance of NCD and set the baseline accuracy that could be attained. 

\item Next, we considered as features for hypoxia classification the aforementioned time and frequency indices that are commonly used in HRV analysis, aiming to evaluate whether they showed some improvement over NCD raw analysis. 

\item Then, we analyzed the performance obtained by using as features the general purpose statistical moments applied to the raw signals. 

\item We also performed feature selection on each group of variables (time and frequency HRV indices and statistical moments), in order to identify the best features and to see whether feature selection could improve classification performance.

\item The experiments continued by evaluating whether the NCD could empower the HRV parameters and statistical moments by obtaining the similarity of the (possibly different length) sequences that result of their application on sliding windows. 

%\item Finally, as the previous experiment only deals with a statistic at a time, we consider feature selection and two strategies (voting classifiers decisions and adding similarities) to combine several statistical moments with the aim of increasing single moment accuracy and yielding more robust results. 

\end{itemize}

\subsection{Raw data analysis}

In this experiment, we analyzed the FHR raw signals in the four time intervals described before. We analyzed three types of signals: a)  including only high quality signals; b) including also medium quality signals; c)  including also medium quality and lost (represented with zeros) signals.

By using NCD, a dissimilarity matrix was created with all pairwise dissimilarities among signals.  We used the software provided by NCD authors~\cite{complearn} to compute the NCD for the signals with three compressor types (zip, bzip2, and lzma). The accuracy was estimated by using LOO cross-validation with a nearest neighbour classifier. 


Best results are summarized in Table~\ref{tab:ncd:raw}, where we can see that high quality signal and the interval from 4 to 3 hours before delivery are the best for prediction accuracy (0.73). In addition, we see that, for the same time interval, the prediction using all the signal is better than using only high and medium qualities, which shows that taking into account lost signals that may occur when the fetus moves, might increase prediction accuracy. Best prediction accuracy for all the patients (4 to 1 hours to delivery interval) was done again considering only high quality signal, but it did not rise above 0.66.

\begin{table}[t]
% DATA from resultadosPrueba_NCD2
\centering
\caption{NCD and nearest neighbor classifier best results for raw signals. ``Quality'' shows the types of signal included in the analysis (High,Medium,Low). ``Interval'' expresses the signal interval in hours to delivery. ``T/C'' shows the number of conTrols and  Cases, respectively. ``Acc.'', ``Sen.'' and ``Spe.'' stand for accuracy, sensitivity and specificity, respectively; and ``Sym.'' shows the method used to make the dissimilarity matrix symmetric.}
\label{tab:ncd:raw}
\resizebox{\linewidth}{!}{\begin{tabular}{cccccccc}
%\toprule
Quality & T/C & Interval & Acc. & Sen. & Spe. & Compressor & Sym.\\ 
%\midrule
H & 13/13 & 4 $\leftrightarrow$  3 & \textbf{0.73} & 0.69 & 0.77 & zip & min\\
H & 13/14 & 3 $\leftrightarrow$  2 & 0.63 & 0.57 & 0.69 & bzip2  & min\\
H & 15/16 & 2 $\leftrightarrow$  1 & 0.58 & 0.75 &0.40  & lzma  & mean\\
H & 15/17 & 4 $\leftrightarrow$  1 & 0.66 & 0.82 & 0.47 & zip & min\\

HM & 13/13 & 4 $\leftrightarrow$  3 & 0.58 & 0.62 & 0.54 & zip & min\\
HM & 13/14 & 3 $\leftrightarrow$  2 & 0.56 & 0.79 & 0.31 & bzip2 & min \\
HM & 15/16 & 2 $\leftrightarrow$  1 & 0.55 & 1.0 & 0.07 & lzma & mean\\
HM & 15/17  & 4 $\leftrightarrow$  1 & 0.56 & 0.59 & 0.53 & lzma & min\\

HML & 13/13 & 4 $\leftrightarrow$  3 &0.66  & 0.77 &0.54  & zip & min\\
HML & 13/14 & 3 $\leftrightarrow$  2 & 0.56 & 0.14 & 1.0 & lzma & min\\
HML & 15/17 & 2 $\leftrightarrow$  1 & 0.53 & 0.76 & 0.27 & lzma & min\\
HML & 15/17 & 4 $\leftrightarrow$  1 & 0.59 & 0.59 & 0.60 & zip & min\\
%\bottomrule
\end{tabular}}
\end{table}

\subsection{Time and frequency HRV indices}

We obtained a set of relevant time HRV descriptors for the considered time intervals. We used all signal qualities, but in this experiment, interpolation was performed on the beats classified as artifacts, as described in Section~\ref{subsec:feat:extraction}. Then, we standardized (zero-mean, unit-variance) each descriptor and combined all of them in a vector.  We benchmarked the following classifiers:  nearest neighbour (NN), $k$ nearest neighbours ($k$-NN), and SVM with linear (SVC) and radial basis function  (RBF-SVC)  kernels. As these features were not designed as dissimilarities, we tried also the SVM,  both to compare its performance with nearest neighbour based classifiers, and to try to extract the best performance of these indices. Table~\ref{tab:time:freq:5min:moments} shows the results of leave one out cross-validation.  The best performance  (0.74 accuracy) was obtained in the 3 $\leftrightarrow$  2 interval by a SVM classifier.


 We repeated the same experiments with the commonly used frequency HRV indices considered above. Results are shown in Table~\ref{tab:time:freq:5min:moments}.  Again, the best results (0.74 accuracy) were obtained in the  3 $\leftrightarrow$  2 interval by k-NN. The combination of the Time and Frequency indices (table not shown) gave a maximum accuracy of 0.70 in the 3 $\leftrightarrow$  2 interval with all classifiers but 1-NN. The best results obtained by these methods provided almost no gain over raw analysis.


\subsection{Statistical Moments}

% + momentos simples.
% prueba_momentos2 y resultado_prueba_momentos2

In order to compute the moments on the records including high and medium signal qualities, we firstly scaled the FHR signal dividing it by the maximum value of each moment for all patients. Then, we calculated raw and central moments of orders $n=\{1,2,\ldots,10\}$ for each patient. Finally, the moments were applied the transformation $x \rightarrow \sqrt[k]{x}$, where $k$ is the order of the moment, and standardized (zero-mean and unit-variance). The results of classifying the records with these moments are also shown in Table~\ref{tab:time:freq:5min:moments}, where a $0.69$ accuracy was obtained in the 4 to 3 hours and in the 4 to 1 hours before delivery intervals. Again, no real gain was obtained by these set of features over the raw analysis.



\begin{table}[t]
%DATA from: prueba_TimeDomain
% DATA from prueba_FreqDomain
% DATA from resultado_prueba_momentos2
\caption{HRV time and frequency indices and statistical moments accuracy for the considered time intervals. SVC and RBF-SVC stand for linear and radial basis kernel Support Vector Classifier, respectively.}
\label{tab:time:freq:5min:moments}
\centering
\begin{tabular}{cccccc}
%\toprule
 Interval          & Features    & 1-NN & $k$-NN & SVC           & RBF-SVC       \\ 
%\midrule
 4 $\leftrightarrow$  3 &Time &0.69 & 0.69   & 0.35          & 0.5           \\ 
 3 $\leftrightarrow$  2 & &0.70 & 0.67   & \textbf{0.74} & \textbf{0.74} \\ 
 2 $\leftrightarrow$  1 & &0.59 & 0.5    & 0.47          & 0.47        \\ 
 4 $\leftrightarrow$  1 & &0.47 & 0.5    & 0.5           & 0.37        \\ 
%\midrule
 4 $\leftrightarrow$  3 &Frequency &0.54 & 0.65          & 0.62 & 0.46    \\
 3 $\leftrightarrow$  2 & &0.56 & \textbf{0.74} & 0.67 & 0.70    \\
 2 $\leftrightarrow$  1 & &0.58 & 0.58          & 0.42 & 0.23    \\
 4 $\leftrightarrow$  1 & &0.5  & 0.5           & 0.53 & 0.44    \\
%\midrule
 4 $\leftrightarrow$3 &Moments &\textbf{0.69}   &   0.62   &   0.46   &   0.58    \\
 3 $\leftrightarrow$2 & &0.22   &   0.63   &   0.59   &   0.52    \\
 2 $\leftrightarrow$1 & &0.23   &  0.065   &   0.48   &   0.29    \\
 4 $\leftrightarrow$1 & & 0.5   &   0.44   &   \textbf{0.69}   &   0.59   \\
%\bottomrule
\end{tabular}
\end{table}




% \subsection{Feature Selection} %% Sin tablas
% Sometimes the learning task can be simplified and even improved by selecting variables. In this section we applied forward selection (FS) to  HRV (time and frequency) indices and to statistical moments. 
% The results of applying  FS to the time HRV indices (table not shown) were in general not better than the obtained without feature selection, maximum overall accuracy decreased to 0.72, which was found in the interval 2 to 1 hours to delivery, where, however, it improved the previous result that was 0.59. FS algorithm consistently selected $stdFHR$  as the unique feature for classification for this case.
% The results (table not shown) for  FS on the frequency HRV indices were slightly better than without feature selection. Best result (0.75 accuracy) was attained on the  4 to 1 hours to delivery interval.  FS algorithm consistently selected $P_{LF}/(P_{MF}+P_{HF})$  as the unique feature for classification for this case. 

% FS on the statistical moments improved the maximum accuracy obtained in Table~\ref{tab:time:freq:5min:moments} with a moderate increase from (0.69 to 0.73). 
% Most selected features for the interval 4 to 3 hours to delivery, where highest accuracy is attained (0.73), were $\mu_4, \mu_8$ and $\mu_9$. 


\subsection{Feature Selection}

Sometimes, the learning task can be simplified and even improved by selecting variables. In this section, we applied forward selection (FS) to  HRV (time and frequency) indices and to statistical moments.   The results of applying  FS to the time HRV indices, shown in Table~\ref{tab:time:freq:5min:moments:featsel}, were in general not better than the obtained without feature selection, as maximum overall accuracy decreased to 0.72, which was found in the interval 2 to 1 hours to delivery, where, however, it improved the previous (0.59) result. FS algorithm consistently selected $stdFHR$  as the unique feature for classification for this case.

The results for FS on the frequency HRV indices, shown in Table~\ref{tab:time:freq:5min:moments:featsel}, were slightly better than without feature selection. FS slightly improved the result in all time intervals. The best result (0.75 accuracy) was attained on the  4 to 1 hours to delivery interval. FS algorithm consistently selected $P_{LF}/(P_{MF}+P_{HF})$  as the single classification feature for this case. 

The results of FS on the statistical moments improved the maximum accuracy obtained in Table~\ref{tab:time:freq:5min:moments} with a moderate increase  (from 0.69 to 0.73). Most selected features for the interval 4 to 3 hours to delivery, where the highest accuracy was attained (0.73), were $\mu_4, \mu_8$, and $\mu_9$. 

\begin{table}[t]
%DATA from: prueba_TimeDomain_featsel
%DATA from: prueba_FreqDomain_featsel
% prueba_momentos_featsel4
% resultado_prueba_momentos_featsel4
\caption{Forward selection accuracy in HRV time and frequency indices and statistical moments for the considered time intervals.}
\label{tab:time:freq:5min:moments:featsel}
\centering
\begin{tabular}{cccccc}
%\toprule
 Interval             &Features  & 1-NN & $k$-NN & SVC           & RBF-SVC       \\ 
%\midrule
 4 $\leftrightarrow$  3 & Time& 0.54  &   0.54  &   0.27   &  0.35 \\
 3 $\leftrightarrow$  2 & & 0.48  &   0.67  &   0.63   &  0.59 \\
 2 $\leftrightarrow$  1 & & \textbf{0.72}  &   0.72  &   0.44   &  0.69 \\
 4 $\leftrightarrow$  1 & & 0.34  &   0.34  &   0.56   &  0.38 \\
%\midrule
 4 $\leftrightarrow$  3 &Frequency &  0.62  &   0.62  &   0.077  &   0.65 \\
 3 $\leftrightarrow$  2 & &  0.52  &   0.59  &   0.74   &   0.67 \\
 2 $\leftrightarrow$  1 & &  0.65  &   0.48  &   0.19   &   0.55 \\
 4 $\leftrightarrow$  1 & &  \textbf{0.75}  &   0.75  &   0.34   &   0.41 \\
5
%\midrule
 4 $\leftrightarrow$  3 & Moments&\textbf{0.73}  &    \textbf{0.73}   &   0.19   &   0.69    \\
 3 $\leftrightarrow$  2 & &0.26  &     0.3   &   0.41   &    0.3    \\
 2 $\leftrightarrow$  1 & &0.42  &    0.39   &   0.42   &   0.42    \\
 4 $\leftrightarrow$  1 & &0.41  &    0.34   &   0.44   &   0.41    \\
%\bottomrule
\end{tabular}
\end{table}
  




\subsection{Time and Frequency Indices in Sliding Windows}

A HRV parameter or a statistical moment provides us with a single value for a signal. If we divide the signal in equal sized windows we got more values, but when comparing  these values with the ones from another patient, both signals should have same length, which will not happen in practice. On the other hand, it is easy to obtain a similarity on signals or sequences of different lengths by using NCD. Therefore, the use of NCD allows us to compare signals from two patients by using the fine grained description provided by the time evolution of a given statistic obtained in a sliding window.
Note that this approach  also evaluates whether  a sequence obtained by sequentially calculating a statistic in a sliding window could have information that can be better exploited by the compressor than the raw signal, which would be(not possible in theory if the  perfect Kolmogorov Complexity could be calculated, but it would be possible in practice as compressors are not perfect.
Therefore, we considered  the calculation of new signals from obtaining these statistics in each time interval of FHR analysis, and we evaluated the performance of obtaining the similarities of these signals for all patients with NCD and by classifying the result with a nearest-neighbour classifier. 


For each time interval, we used the NCD to analyze the time and frequency indices in 5-minute sliding windows, where a window is only considered if its data had not too many artifacts (see Section~\ref{subsec:feat:extraction}). For each parameter, a sequence was constructed for each patient by  concatenating the parameter value for each sliding window of the FHR signal. An NCD matrix for each parameter was later constructed by obtaining the similarities among all pairs of patient sequences, and  the accuracy of a nearest neighbour classifier was estimated by leave-one-out cross-validation.

Table~\ref{tab:ncd:time:freq:moments:5min} shows the best individual accuracies of the HRV time indices in each analysis interval. The best result (0.70 accuracy, 0.86 sensitivity and 0.54 specificity) was again in the  3 $\leftrightarrow$  2 interval by using the LTI index. We combined the 4 time indices by voting, and the best result gave  an accuracy of 0.66 with sensitivity of 0.76 and specificity of 0.53 in the 4 $\leftrightarrow$  1 interval, which was better that the result provided by adding the dissimilarity matrices.


Table~\ref{tab:ncd:time:freq:moments:5min} also shows the best individual accuracies of the HRV frequency indices. The best result (0.77 accuracy, 0.77 specificity and 0.77 specificity) was in the  4 $\leftrightarrow$  3 interval by using $P_{LF} / (P_{MF}+P_{HF})$. Different indices seemed to be the most informative in each interval. We combined the 6 frequency indices by adding the dissimilarity matrices, and the best result gave an accuracy of 0.69 with sensitivity of 0.53  and specificity of 0.85 in the 4 $\leftrightarrow$  3 interval.

\begin{table}[t]
% DATA from prueba_momentos_5min.mat
\caption{NCD and nearest neighbour classifier best results for HRV time and frequency indices and statistical moments in 5-minute sliding-windows signals.}
\label{tab:ncd:time:freq:moments:5min}
\centering
\begin{tabular}{cccccccc}
%\toprule
 Interval    &Features   & Acc.     & Sen. & Spe. & Feature  & Comp. & Sym. \\ 
%\midrule
 4 $\leftrightarrow$  3 & Time & 0.62 &0.69 & 0.54 &  $sdFHR$ & bzip2 & min\\
 3 $\leftrightarrow$  2 & &\textbf{0.70} &0.86 & 0.54 &  LTI & lzma & min\\
 2 $\leftrightarrow$  1 & & 0.66 &0.65 & 0.67 & \mfhr & bzip2 & min\\
 4 $\leftrightarrow$  1 & & 0.69 & 0.76 & 0.6 & \mfhr & bzip2 & min\\
%\midrule
 4 $\leftrightarrow$  3 & Frequency& \textbf{0.77} & 0.77 & 0.77 & $\frac{P_{LF}}{P_{MF}+P_{HF}}$ & bzip2 & min\\
 3 $\leftrightarrow$  2 & & 0.59 & 0.64 & 0.54 & $P_{VLF}$ & bzip2 & min\\
 2 $\leftrightarrow$  1 & & 0.69 & 0.65 & 0.73 & $P_{HF}$ &  bzip2 & min\\
 4 $\leftrightarrow$  1 & & 0.69 & 0.88 & 0.47 &$P_{LF}$ & lzma & mean\\  
%\midrule
%acc3
 4 $\leftrightarrow$  3 & Moments& \textbf{0.88} & 0.92 & 0.85 & $\mu_3$ & lzma       & min    \\
%acc2
 3 $\leftrightarrow$  2 & & 0.70          & 0.64 & 0.77 & $\mu_2$ & bzip2      & min    \\ % tambien (\mu_2,bzip2, mean) y (M_8, zip, mean)
% acc1
 2 $\leftrightarrow$  1 & & 0.77          & 0.81 & 0.73 & $M_4$  & zip        & mean    \\ % tambien (\mu_9,lzma,min)
%acc4
 4 $\leftrightarrow$  1 & & 0.81          & 0.82 & 0.80 & $M_4$  & lzma       & mean    \\ 
%\bottomrule
\end{tabular}
\end{table}


\subsection{Moments in Sliding Windows}

In this experiment, we used high and medium signal qualities and 5 minutes sliding windows. For each window, we computed raw and central moments of orders $n \in \{1,2,\ldots,10\}$. Then, for each moment of order $n$, the results of all windows were concatenated to obtain the new signal $\bs_{i,n}$ that provided a description of the patient $i$. Later, this signal was transformed as $\bar{\bs}_{i,n}=\sqrt[n]{\bs_{i,n}}/A_n$ where  $A_n$ is the maximum value of the signals $\{\bar{\bs}_{i,n}\}_{i=1}^{N_T}$. Then, the NCD pairwise distances were obtained for pairs $(\bar{\bs}_{i,n}, \bar{\bs}_{j,n})$ and accuracies were estimated using leave-one-out cross-validation with a nearest neighbour classifier.

The results are summarized in Table~\ref{tab:ncd:time:freq:moments:5min}. The best predictive interval was the 4 to 3 hours to delivery. The best accuracy for individual moments gave an accuracy of 0.88  (23 out of 26), a sensitivity of 0.92 (12 out of 13) and a specificity of 0.85 (11 out of 13). In addition, we noted the good performance of the 4 to 1 hours to delivery interval, which  can be applied to any record of our database, 0.81 accuracy (26 out of 32), 0.82 sensitivity (14 out of 17) and 0.80 specificity (12 out of 15).




%\subsection{Voting moments in sliding windows}
%One possibility to increase accuracy is to combine the decisions of independent experts. An expert, in this case, is a classifier that uses the NCD similarities calculated from one statistical moment. Table~\ref{tab:ncd:time:freq:moments:5min} shows the best moments for each time interval. We combined decisions from the intervals 4 to 3 hours,  3 to 2 and 2 to 1 hours to delivery for the records that have signal in the three interval (25 in our case). 
% Feature selection with the methods presented above was applied to the training data inside each LOO cross-validation step to select the decisions (moments) to be combined. 
%The results for these methods with a nearest neighbor classifier are summarized in Table~\ref{tab:fs:momentos:comp:voting:adding}. We note that there were 120 variables (20 moments, 3 compressors, and 2 ways of considering the dissimilarity matrix) for the one-hour intervals (360 for the three hours interval) and 25 instances to classify, which made the feature selection problem hard and  prone to overfitting. The maximum number variables to be searched by forward selection was limited to five, in order to limit the overfitting effect.
%Best result, 0.73 accuracy,  was provided  by the 4 to 3 hours to delivery interval. This result did not reach the accuracies presented above, and the feature selection methods were not able to find the best variables (maximum accuracy could reach 0.96 picking variables by hand). The more selected moments in each loop step are detailed  in decreasing order of appearance as follows: $\mu_3$, $M_1$, $M_{10}$, $M_9$, $\mu_4$, $\mu_6$ (variables only selected once are omitted). To compute the nearest neighbor, the \emph{min} matrix type was selected in the majority of cases. The type or compressor more selected was lzma, then bzip2, and finally zip.


%\subsection{Combining moment dissimilarity matrices}

%Voting can be understood as a simple combination of hard decisions, because we ignore how \emph{sure} the classifier is about its vote. Another possibility is to combine soft decisions, which in our case is equivalent to the addition of the dissimilarity matrices previous to   the nearest neighbor classification (the moment matrices to be added are determined by feature selection). By using soft decisions, a case that is going to be wrongly classified by one variable could be corrected by another variable that has a greater confidence about this case. It may also happen the converse, a confident but wrong opinion can hinder a good classification. To compare both methods, we present the results for the addition of dissimilarity matrices in Table~\ref{tab:fs:momentos:comp:voting:adding}. Best results (0.88 accuracy, 0.85 sensitivity, 0.92 specificity)  were given for the FS feature selection method, in the 4 to 1 hours to delivery interval. Most selected variables were in this case: $\mu_3$, $M_1$ ,$M_{10}$ and $M_9$ (variables only selected once are omitted). All variables were selected from the min type of matrix. The type or compressor more selected was bzip2, then lzma, and finally zip.



%\begin{table}[t]
%  \caption{LOO cross-validation accuracy given by nearest neighbor classifier and two feature selection procedures for moment signals in 5 minutes sliding windows combining variables by voting classifier decisions or adding dissimilarity matrices.}
%  \label{tab:fs:momentos:comp:voting:adding}
%  \centering
%  \begin{tabular}{ccccc}
%\toprule
%                        & \multicolumn{2}{c}{Voting} & %\multicolumn{2}{c}{Adding}                                        \\
% \cmidrule(r){2-3}   \cmidrule(r){4-5}
%                                               & ATS   & FSCV2          & ATS   %& FSCV2 \\
%\midrule
% 4 $\leftrightarrow$  3                        & 0.58  & \textbf{0.73}  & 0.81  %& 0.58  \\
% 3 $\leftrightarrow$  2                        & 0.41  & 0.41           & 0.37  %& 0.63  \\
% 2 $\leftrightarrow$  1                        & 0.61  & 0.71           & 0.48  %& 0.55  \\
% 4 $\leftrightarrow$  1                        & 0.52  & 0.68           & 0.76  %& \textbf{0.88}  \\
%\bottomrule
%  \end{tabular}
%\end{table}



